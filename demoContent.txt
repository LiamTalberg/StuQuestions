[SOUND] This lecture is about
natural language content analysis.
As you see from this picture,
this is really the first step
to process any text data.
Text data are in natural languages.
So, computers have to understand
natural languages to some extent in
order to make use of the data, so
that's the topic of this lecture.
We're going to cover three things.
First, what is natural language
processing, which is a main technique for
processing natural language
to obtain understanding?
The second is the State of the Art in NLP,
which stands for
natural language processing.
Finally, we're going to cover the relation
between natural language processing and
text retrieval.
First, what is NLP?
Well, the best way to
explain it is to think about,
if you see a text in a foreign
language that you can't understand.
Now, what you have to do in
order to understand that text?
This is basically what
computers are facing.
Right?
So, looking at the simple sentence like,
a dog is chasing a boy on the playground.
We don't have any problems
understanding this sentence, but
imagine what the computer would have
to do in order to understand it.
For in general,
it would have to do the following.
First, it would have to know dog is
a noun, chasing's a verb, et cetera.
So, this is a code lexile analysis or
part of speech tagging.
And, we need to pick out the,
the syntaxing categories of those words.
So, that's a first step.
After that, we're going to figure
out the structure of the sentence.
So for example, here it shows that a and
dog would go together
to form a noun phrase.
And, we won't have dog and
is to go first, right.
And, there are some structures
that are not just right.
But, this structure shows what we might
get if we look at the sentence and
try to interpret the sentence.
Some words would go together first, and
then they will go together
with other words.
So here, we show we have noun phrases
as intermediate components and
then verb phrases.
Finally, we have a sentence.
And, you get this structure, we need to
do something called a syntactic analysis,
or parsing.
And, we may have a parser,
a computer program that would
automatically create this structure.
At this point, you would know
the structure of this sentence, but
still you don't know
the meaning of the sentence.
So, we have to go further
through semantic analysis.
In our mind,
we usually can map such a sentence to what
we already know in our knowledge base.
And for example, you might imagine
a dog that looks like that,
there's a boy and
there's some activity here.
But for computer,
will have to use symbols to denote that.
All right.
So, we would use the symbol
d1 to denote a dog.
And, b1 to denote a boy, and then p1
to denote the playground, playground.
Now, there is also a chasing
activity that's happening here, so
we have the relation chasing here,
that connects all these symbols.
So, this is how a computer would obtain
some understanding of this sentence.
Now from this representation, we could
also further infer some other things,
and we might indeed, naturally think
of something else when we read text.
And, this is call inference.
So for example, if you believe
that if someone's being chased and
this person might be scared.
All right.
With this rule,
you can see computers could also
infer that this boy may be scared.
So, this is some extra knowledge
that you would infer based on
some understanding of the text.
You can even go further to understand the,
why the person said this sentence.
So, this has to do with
the use of language.
All right.
This is called pragmatic analysis.
In order to understand the speech
actor of a sentence, all right,
we say something to
basically achieve some goal.
There's some purpose there and
this has to do with the use of language.
In this case, the person who said
the sentence might be reminding
another person to bring back the dog.
That could be one possible intent.
To reach this level of understanding,
we would require all these steps.
And, a computer would have to go
through all these steps in order to
completely understand this sentence.
Yet, we humans have no
trouble with understand that.
We instantly, will get everything,
and there is a reason for that.
That's because we have a large
knowledge base in our brain, and
we use common sense knowledge
to help interpret the sentence.
Computers, unfortunately,
are hard to obtain such understanding.
They don't have such a knowledge base.
They are still incapable of doing
reasoning and uncertainties.
So, that makes natural language
processing difficult for computers.
But, the fundamental reason why the
natural language processing is difficult
for computers is simple because natural
language has not been designed for
computers.
They, they, natural languages
are designed for us to communicate.
There are other languages designed for
computers.
For example, program languages.
Those are harder for us, right.
So, natural languages is designed to
make our communication efficient.
As a result,
we omit a lot of common sense knowledge
because we assume everyone
knows about that.
We also keep a lot of ambiguities
because we assume the receiver, or
the hearer could know how to
discern an ambiguous word,
based on the knowledge or the context.
There's no need to invent a different
word for different meanings.
We could overload the same word with
different meanings without the problem.
Because of these reasons,
this makes every step in natural language
of processing difficult for computers.
Ambiguity's the main difficulty, and
common sense reasoning is often required,
that's also hard.
So, let me give you some
examples of challenges here.
Conceded the word-level ambiguities.
The same word can have different
syntactical categories.
For example,
design can be a noun or a verb.
The word root may have multiple meanings.
So, square root in math sense,
or the root of a plant.
You might be able to
think of other meanings.
There are also syntactical ambiguities.
For example, the main topic of this
lecture, natural language processing,
can actually be interpreted in two ways,
in terms of the structure.
Think for a moment and
see if you can figure that out.
We usually think of this as
processing of natural languages, but
you could also think of this as you say,
language process is natural.
Right.
So, this is example of syntatic ambiguity.
Where we have different
structures that can be
applied to the same sequence of words.
Another example of ambiguous
sentence is the following,
a man saw a boy with a telescope.
Now, in this case, the question is,
who had the telescope?
All right, this is called a prepositional
phrase attachment ambiguity,
or PP attachment ambiguity.
Now, we generally don't have a problem
with these ambiguities because we have
a lot of background knowledge to
help us disintegrate the ambiguity.
Another example of difficulty
is anaphora resolution.
So, think about the sentence like John
persuaded Bill to buy a TV for himself.
The question here is,
does himself refer to John or Bill?
So again, this is something that
you have to use some background or
the context to figure out.
Finally, presupposition
is another problem.
Consider the sentence,
he has quit smoking.
Now this obviously
implies he smoked before.
So, imagine a computer wants to understand
all the subtle differences and meanings.
They would have to use a lot of
knowledge to figure that out.
It also would have to maintain a large
knowl, knowledge base of odd meanings of
words and how they are connected to our
common sense knowledge of the word.
So this is why it's very difficult.
So as a result we are still not perfect.
In fact, far from perfect in understanding
natural languages using computers.
So this slide sort of gives a simplified
view of state of the art technologies.
We can do part of speech
tagging pretty well.
So, I showed minus 7% accuracy here.
Now this number is obviously
based on a certain data set, so
don't take this literally.
All right, this just shows that
we could do it pretty well.
But it's still not perfect.
In terms of parsing,
we can do partial parsing pretty well.
That means we can get noun phrase
structures or verb phrase structure, or
some segment of the sentence understood
correctly in terms of the structure.
And, in some evaluation
results we have seen about 90%
accuracy in terms of partial
parsing of sentences.
Again, I have to say, these numbers
are relative to the data set.
In some other data sets,
the numbers might be lower.
Most of existing work has been
evaluated using news data set.
